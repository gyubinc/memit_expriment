{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured GPU: 1\n",
      "Available GPUs: 1\n",
      "Current GPU: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# GPU 1번만 사용하도록 설정\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "# 확인을 위해 현재 설정된 환경 변수 출력\n",
    "print(\"Configured GPU:\", os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "\n",
    "import torch\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "# torch.cuda.set_device(1)\n",
    "print(\"Current GPU:\", torch.cuda.current_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from transformers import set_seed as hf_set_seed\n",
    "\n",
    "# 랜덤 시드 값 설정\n",
    "SEED = 42\n",
    "\n",
    "# 파이썬 내장 랜덤 모듈의 시드 고정\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy의 랜덤 시드 고정\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch의 랜덤 시드 고정\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # 멀티 GPU 사용 시 필요\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Hugging Face Transformers의 랜덤 시드 고정\n",
    "hf_set_seed(SEED)\n",
    "\n",
    "# 운영체제 레벨에서 랜덤 시드 고정 (멀티 쓰레딩 등에서 활용)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "from util.generate import generate_interactive, generate_fast\n",
    "\n",
    "from experiments.py.demo import demo_model_editing, stop_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2-xl\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 1600,\n",
       "  \"n_head\": 25,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 48,\n",
       "  \"n_positions\": 1024,\n",
       "  \"output_past\": true,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.23.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        low_cpu_mem_usage=False,\n",
    "        torch_dtype=(torch.float16 if \"20b\" in MODEL_NAME else None),\n",
    "    ).to(\"cuda\"),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
    ")\n",
    "tok.pad_token = tok.eos_token\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} plays the sport of\",\n",
    "        \"subject\": \"Serena Williams\",\n",
    "        \"target_new\": {\"str\": \"volleyball\"}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} is famous for playing\",\n",
    "        \"subject\": \"Lionel Messi\",\n",
    "        \"target_new\": {\"str\": \"basketball\"}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} competes in\",\n",
    "        \"subject\": \"Michael Phelps\",\n",
    "        \"target_new\": {\"str\": \"sailing\"}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} is a professional\",\n",
    "        \"subject\": \"Tom Brady\",\n",
    "        \"target_new\": {\"str\": \"cricket\"}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} has won championships in\",\n",
    "        \"subject\": \"Roger Federer\",\n",
    "        \"target_new\": {\"str\": \"badminton\"}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} is a world-class\",\n",
    "        \"subject\": \"Simone Biles\",\n",
    "        \"target_new\": {\"str\": \"archery\"}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} plays the sport of\",\n",
    "        \"subject\": \"Cristiano Ronaldo\",\n",
    "        \"target_new\": {\"str\": \"hockey\"}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} is best known for\",\n",
    "        \"subject\": \"Usain Bolt\",\n",
    "        \"target_new\": {\"str\": \"football\"}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} excels in\",\n",
    "        \"subject\": \"Tiger Woods\",\n",
    "        \"target_new\": {\"str\": \"rugby\"}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} has been a top player in\",\n",
    "        \"subject\": \"Novak Djokovic\",\n",
    "        \"target_new\": {\"str\": \"table tennis\"}\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompts = [\n",
    "    'There',\n",
    "    'It',\n",
    "    'She',\n",
    "    'He',\n",
    "    'When',\n",
    "    'Where',\n",
    "    'Who',\n",
    "    'What',\n",
    "    'How',\n",
    "    'Why',\n",
    "    'For',\n",
    "    'Then',\n",
    "    \"Let's\",\n",
    "    'People',\n",
    "    'Person'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALG_NAME = \"MEMIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sat May  4 13:14:28 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:31:00.0 Off |                    0 |\n",
      "| 57%   83C    P2   267W / 300W |   5467MiB / 46068MiB |     77%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:4B:00.0 Off |                    0 |\n",
      "| 30%   51C    P2    90W / 300W |  14053MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1182591      C   python3                          5464MiB |\n",
      "|    1   N/A  N/A   1179084      C   .../envs/memit_ex/bin/python    14050MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model restored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:26<00:25,  5.18s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for k, v in orig_weights.items():\n",
    "            nethook.get_parameter(model, k)[...] = v\n",
    "    print(\"Original model restored\")\n",
    "except NameError as e:\n",
    "    print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "# Execute rewrite\n",
    "\n",
    "model_new, orig_weights, answer, pre, post = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=ALG_NAME, num=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Request 1 : [Serena Williams plays the sport of] -> [volleyball]', 'Request 2 : [Lionel Messi is famous for playing] -> [basketball]', 'Request 3 : [Michael Phelps competes in] -> [sailing]', 'Request 4 : [Tom Brady is a professional] -> [cricket]', 'Request 5 : [Roger Federer has won championships in] -> [badminton]', 'Request 6 : [Simone Biles is a world-class] -> [archery]', 'Request 7 : [Cristiano Ronaldo plays the sport of] -> [hockey]', 'Request 8 : [Usain Bolt is best known for] -> [football]', 'Request 9 : [Tiger Woods excels in] -> [rugby]', 'Request 10 : [Novak Djokovic has been a top player in] -> [table tennis]']\n"
     ]
    }
   ],
   "source": [
    "formatted_requests = []\n",
    "for i, req in enumerate(request):\n",
    "    formatted_request = f\"Request {i+1} : [{req['prompt'].format(req['subject'])}] -> [{req['target_new']['str']}]\"\n",
    "    formatted_requests.append(formatted_request)\n",
    "\n",
    "print(formatted_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로그 파일이 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "if not os.path.exists('KE_result'):\n",
    "    os.makedirs('KE_result')\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "filename = f\"KE_result/{current_datetime}.txt\"\n",
    "with open(filename, 'w') as file:\n",
    "    for request in formatted_requests:\n",
    "        file.write(request+'\\n')\n",
    "    \n",
    "    for text in answer:\n",
    "        file.write(text + '\\n')\n",
    "\n",
    "print(\"로그 파일이 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# from torchtext.datasets import AG_NEWS\n",
    "# from torchtext.data.functional import to_map_style_dataset\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetForSequenceClassification.from_pretrained('results/checkpoint-2345')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # 평가 모드 설정\n",
    "\n",
    "# 예측을 수행하는 함수\n",
    "def classify_texts(texts):\n",
    "    predictions = []\n",
    "    for text in texts:\n",
    "        # 텍스트를 토크나이저로 인코딩\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "        \n",
    "        # GPU 사용 가능 시 GPU로 데이터 이동\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            model.to('cuda')\n",
    "        \n",
    "        # 모델로 예측 수행\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_class_id = logits.argmax().item()\n",
    "            predictions.append(predicted_class_id)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {0: 'world',\n",
    "    1: 'sports',\n",
    "    2: 'business',\n",
    "    3: 'science'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science: 10\n",
      "sports: 2\n",
      "world: 1\n",
      "business: 2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "predicted_classes = classify_texts(pre)\n",
    "\n",
    "pre_result = [topics[item] for item in predicted_classes]\n",
    "\n",
    "# 예측 결과 출력\n",
    "text_count = Counter(pre_result)\n",
    "\n",
    "for text, count in text_count.items():\n",
    "    print(f'{text}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science: 8\n",
      "sports: 1\n",
      "world: 5\n",
      "business: 1\n"
     ]
    }
   ],
   "source": [
    "# 텍스트를 분류\n",
    "predicted_classes = classify_texts(post)\n",
    "\n",
    "post_result = [topics[item] for item in predicted_classes]\n",
    "\n",
    "text_count = Counter(post_result)\n",
    "\n",
    "for text, count in text_count.items():\n",
    "    print(f'{text}: {count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit_ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
